{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Agnostic Neural Networks\n",
    "\n",
    "2010.01.12. <br>\n",
    "Jaehwi Park"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    " 생물학적으로 조숙한 종들은 태어나자마자 몇 가지 task를 할 수 있는 능력이 있다고 합니다. 이에 영감을 받아 본 논문은 주어진 task를 random-sampled weight로 해결할 수 있는 architectures를 찾는 방법을 제시합니다.\n",
    " 수십년간의 신경망 연구들은 다양한 task들에 특화된 CNN, LSTM 등의 network block들을 고안해냈습니다. 저자들은 그런 block들의 본질적인 문제 해결능력을 믿고 random-weights CNN과 LSTM을 이용하여 weight-agnostic 신경망을 찾아냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MNIST](MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 가중치의 중요성을 \"deemphasizing\"하여 원하는 신경망 구조를 찾을 수 있다고 합니다. \"deemphasizing\"은 다음과 같이 가능합니다.\n",
    "  1. 모든 네트워크에서 단 하나의 공유되는 weight를 사용\n",
    "  2. 하나의 weight parameter를 사용하는 신경망들을 평가\n",
    "\n",
    "위와 같은 방법은 전통적으로, 모형을 고정하고 weight를 탐색하는 방법과 완벽히 상반됩니다. 이렇게 학습시킨 모형으로 MNIST에서 92% 정도의 test accuracy를 얻을 수 있었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Related Work\n",
    "\n",
    " 먼저 **Architecture Search**와 관련이 깊습니다.본 연구에서 NAS를 위해 \"NEAT(Evolving neural networks through augmenting topologies, MIT, 2002)\" 방법을 활용했다고 합니다. 또한 distibution에서 weights를 sampling 하는 **Bayesian Neural Networks**와도 관련이 있습니다. 본 연구에서는 fixed uniform distribution with zero mean에서 weight를 sampling한 것으로 볼 수 있습니다. 그리고, **Algorithmic Information Theory**에서 network capacity에 관해 연구하는 것처럼, 본 연구에서는 특정 task를 수행하기 위한 minial architecture를 탐색합니다. 마지막으로 **Network Pruning**에서 가지치기를 하는 것과는 반대로 본 논문에서는 block들을 adding 하는 task를 수행합니다. Full-network가 존재하는 상황에서 pruning이 가능하므로 본 연구에 직접적으로 적용하는 것은 불가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weight Agnostic Neural Network Search\n",
    "\n",
    "Weight Agnostic Network(WANN)는 Neural Network Search(NAS)와는 기본적으로 다릅니다. NAS는 Arhcitecture를 찾고, training까지 거쳐야 원하는 task를 수행할 수 있습니다. weights 자체가 solution이 되는 것입니다. WANN에서는 모듈들의 본질적인 능력을 믿고 architecture를 찾는 것이 핵심입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WANN에서 weights는 그 중요성이 최소화 되어야 합니다. 그러기 위해서 weight를 random-distribution에서 sampling 할 뿐, weights 값에 따라 network가 평가되서는 안됩니다. a single-shared weight를 사용하므로 network의 성능은 오로지 network-topology에서 비롯된다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure2](Figure2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure3](Figure3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WANN는 \"NEAT\" 방법론에서 weight search 부분을 제외하고 architecture 만을 탐색합니다.\n",
    "\n",
    "  1. 먼저 Sparsely connected network를 만들어 냅니다.\n",
    "  2. 네트워크를 평가하고, 우수한 네트워크를 선정합니다.\n",
    "    - 네트워크 평가에는 여러가지 weight들이 모든 네트워크에 적용됩니다.\n",
    "    - 본 연구에서는 fixed set을 사용했습니다. {-2, -1, -.5, .5, 1, 2}\n",
    "    - 모든 weights에 대한 평가 결과의 평균으로 네트워크를 평가합니다.\n",
    "    - connection cost를 적용하여 performance 평가시 네트워크의 크기도 함께 고려합니다.\n",
    "    - 네트워크는 총 세 가지 평가 기준이 있습니다. {평균성능, 최고성능, connection수}\n",
    "    - 80%는 평균성능을 사용하고, 나머지 20%는 최고성능 또는 connection수로 평가했습니다.\n",
    "  3. 우수한 네트워크에 Insert / Add / Change 중 한 가지 방법을 적용하여 새로운 네트워크를 만들어 냅니다.\n",
    "    - Activation Function은 기본적으로 임의로 선택됩니다.\n",
    "    - Activation Function은 linear, step, sin/cosine, Gaussian, tanh, sigmoid, inverse, abs, ReLU, sinusoid 등 다양한 것들이 가능합니다.\n",
    "  5. 위의 방법을 반복합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Table1](Table1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WANN과 기존 RL과의 비교 결과 테이블에서 다음과 같은 Insights를 얻을 수 있습니다.\n",
    "  - WANN은 topology 자체의 능력을 보유하고 있기 때문에, Random Weights에서 RL보다 좋은 결과를 보여줍니다.\n",
    "  - WANN도 완벽하게 weight-independent하지 못합니다. 특정 weights에서는 task를 잘 수행하지 못하는 모습이 관찰됐습니다. 특히 0과 가까운 값에서는 전달되는 정보 값이 적어 성능이 좋지 않았습니다.\n",
    "  - WANN에서 weight의 magnitude는 중요하지 않았고, 중요한 것은 sign(부호)였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure4](Figure4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network가 너무 복잡하지 않은 경우, input-output 관계를 직접 파악할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습 분야에서의 좋은 결과가 Classification에서도 가능한지를 체크했습니다.\n",
    "\n",
    "![Figure6](Figure6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single depth network와 유사한 성능을 보였지만, 2단 CNN보다도 훨씬 안좋은 성능을 보여줍니다.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Materials\n",
    "\n",
    "논문 이해에 도움이 되는 부분만 추가했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.1 Searching for network architecture using a single weight rather than range of weights\n",
    "\n",
    "한 가지 weight로 architecture를 탐색했더니, 조금만 그 값이 바뀌어도 성능이 엉망이 됐다고 합니다. 그래서 학습을 여러가지 single-shared weights로 진행했다고 합니다. 그리고 최적의 weight값은 보통 training set 외에 있다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.2  Searching for network architecture using random weights for each connection\n",
    "\n",
    "저자들이 처음 시도한 방법입니다. 잘 작동하는 줄 알았지만, code bug로 모든 weight가 동일하게 설정된 것을 발견했다고 합니다. code를 고치고 실험했더니 잘 안되는 것을 발견하고, a single-shared weight를 사용하는 방향으로 연구를 진행했다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.3. Adding noise to the single weight values\n",
    "\n",
    "Each weights가 다르도록 Gaussian noise를 추가해서 실험을 진행했지만, 크게 달라지지 않고 큰 noise에서는 오히려 성능이 하락하는 것을 확인했다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.4. Using backpropagation to fine-tune weights of a WANN\n",
    "\n",
    "MNIST 실험을 할 때, JAX와 같은 autograd packages를 사용해서 개별 weights를 update해서 성능이 좋아졌다고 합니다. 그런데 CMA-ES, population-based REINFORCE 방법론이 WANN에 더 적합한 것을 확인했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.5. Why did you choose to use many different activation functions in the same network? Why not just ReLU? Wouldn’t too many activations break biological plausibility?\n",
    "\n",
    "실험을 많이 진행해본 것은 아니지만, 다양한 Activation들이 본 연구의 key라고 생각한다고 합니다. 물론 ReLU, sigmoid만으로 가능할 수도 있겠지만, 그러면 네트워크가 너무 복잡해질 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
